{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TeddyNote Parser API 클라이언트 테스트\n",
    "\n",
    "이 노트북은 `parser_client.py`에 구현된 `TeddyNoteParserClient` 클래스를 사용하여 TeddyNote Parser API와 상호작용하는 과정을 보여줍니다.\n",
    "\n",
    "주요 단계:\n",
    "1. 클라이언트 초기화 및 API 연결 확인\n",
    "2. PDF 파일 업로드 및 파싱 작업 요청\n",
    "3. 작업 상태 확인\n",
    "4. 완료된 파싱 결과 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -U teddynote-parser-api pandas langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# API 서버 설정\n",
    "API_URL = \"http://localhost:9990\"  # API 서버 주소\n",
    "\n",
    "# PDF 파일 경로\n",
    "PDF_FILE_PATH = \"data/2210.03629v3.pdf\"  # PDF 파일 경로\n",
    "\n",
    "# 결과 저장 디렉토리\n",
    "OUTPUT_DIR = \"client_test_results\"\n",
    "RESULTS_DIR = Path(OUTPUT_DIR)\n",
    "RESULTS_DIR.mkdir(exist_ok=True)  # 결과 저장 디렉토리 생성\n",
    "\n",
    "# API 키 설정 (환경 변수에서 로드하거나 직접 지정)\n",
    "UPSTAGE_API_KEY = os.environ.get(\"UPSTAGE_API_KEY\", \"your_upstage_api_key\")\n",
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\", \"your_openai_api_key\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 클라이언트 초기화 및 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from teddynote_parser_client.client import TeddyNoteParserClient\n",
    "\n",
    "# 로깅 설정\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "logger = logging.getLogger(\"teddynote_parser_client\")\n",
    "\n",
    "# TeddyNoteParserClient 인스턴스 생성\n",
    "client = TeddyNoteParserClient(\n",
    "    api_url=API_URL,  # API 서버 URL\n",
    "    upstage_api_key=UPSTAGE_API_KEY,  # UPSTAGE LLM API 키\n",
    "    openai_api_key=OPENAI_API_KEY,  # OpenAI API 키 (대체 LLM으로 사용)\n",
    "    batch_size=50,  # 한 번에 처리할 PDF 페이지 수\n",
    "    test_page=None,  # 테스트용 페이지 제한 (None: 전체 페이지 처리)\n",
    "    language=\"Korean\",  # 문서 언어 설정\n",
    "    include_image=True,  # 결과에 이미지 포함 여부\n",
    "    logger=logger,  # 로깅에 사용할 로거\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "서버에 정상 동작중인지 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-10 03:53:49,860 - teddynote_parser_client - INFO - API 서버가 정상적으로 응답했습니다.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ API 서버가 정상적으로 실행 중입니다. 상태: {'status': 'ok', 'timestamp': '2025-03-10T03:53:49.858499'}\n"
     ]
    }
   ],
   "source": [
    "# API 서버 상태 확인\n",
    "try:\n",
    "    health_status = client.health_check()\n",
    "    print(f\"✅ API 서버가 정상적으로 실행 중입니다. 상태: {health_status}\")\n",
    "    api_available = True\n",
    "except Exception as e:\n",
    "    print(f\"❌ API 서버에 접속할 수 없습니다: {e}\")\n",
    "    print(\"API 서버가 실행 중인지 확인해주세요.\")\n",
    "    api_available = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. PDF 파일 업로드 및 파싱 작업 요청"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-10 03:53:51,338 - teddynote_parser_client - INFO - 파일 '2210.03629v3.pdf'에 대한 파싱 작업 요청 중...\n",
      "2025-03-10 03:53:51,339 - teddynote_parser_client - INFO - 파싱 옵션: 언어=English, 이미지 포함=True, 배치 크기=50, 처리 페이지 수=5\n",
      "2025-03-10 03:53:51,359 - teddynote_parser_client - INFO - 파싱 작업이 시작되었습니다. 작업 ID: 3d541e4d-21ba-4b1c-9c69-d4c82d346422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📄 파일 'data/2210.03629v3.pdf'에 대한 파싱 작업을 요청합니다...\n",
      "✅ 파싱 작업이 시작되었습니다!\n",
      "📝 작업 ID: 3d541e4d-21ba-4b1c-9c69-d4c82d346422\n",
      "📝 상태: pending\n",
      "📝 메시지: PDF 파싱 작업이 시작되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# PDF 파일 파싱 요청\n",
    "if api_available:\n",
    "    try:\n",
    "        print(f\"📄 파일 '{PDF_FILE_PATH}'에 대한 파싱 작업을 요청합니다...\")\n",
    "        # pdf_path: 파싱할 PDF 파일 경로\n",
    "        # batch_size: 한 번에 처리할 페이지 수(기본값: 30, 최대값: 100)\n",
    "        # language: Entity 파싱 언어\n",
    "        # test_page: 테스트용으로 처음 ~ 지정한 페이지까지만 처리(기본값: None - 모든 페이지 처리)\n",
    "        # include_image: 결과에 이미지 포함 여부(기본값: True)\n",
    "        parse_result = client.parse_pdf(\n",
    "            pdf_path=PDF_FILE_PATH,\n",
    "            batch_size=50,\n",
    "            language=\"English\",\n",
    "            test_page=5,\n",
    "            include_image=True,\n",
    "        )\n",
    "\n",
    "        job_id = parse_result[\"job_id\"]\n",
    "        print(f\"✅ 파싱 작업이 시작되었습니다!\")\n",
    "        print(f\"📝 작업 ID: {job_id}\")\n",
    "        print(f\"📝 상태: {parse_result['status']}\")\n",
    "        print(f\"📝 메시지: {parse_result['message']}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 파싱 작업 요청 실패: {e}\")\n",
    "        job_id = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 작업 상태 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-10 03:53:53,731 - teddynote_parser_client - INFO - 작업 ID '3d541e4d-21ba-4b1c-9c69-d4c82d346422'의 완료 대기 중...\n",
      "2025-03-10 03:53:53,731 - teddynote_parser_client - INFO - 상태 확인 간격: 2초, 최대 시도 횟수: 30회\n",
      "2025-03-10 03:53:53,736 - teddynote_parser_client - INFO - 작업 ID '3d541e4d-21ba-4b1c-9c69-d4c82d346422'의 현재 상태: processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "작업 ID: 3d541e4d-21ba-4b1c-9c69-d4c82d346422에 대한 상태를 확인하고 완료될 때까지 대기합니다...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-10 03:53:55,748 - teddynote_parser_client - INFO - 작업 ID '3d541e4d-21ba-4b1c-9c69-d4c82d346422'의 현재 상태: processing\n",
      "2025-03-10 03:53:57,768 - teddynote_parser_client - INFO - 작업 ID '3d541e4d-21ba-4b1c-9c69-d4c82d346422'의 현재 상태: processing\n",
      "2025-03-10 03:53:59,776 - teddynote_parser_client - INFO - 작업 ID '3d541e4d-21ba-4b1c-9c69-d4c82d346422'의 현재 상태: processing\n",
      "2025-03-10 03:54:01,787 - teddynote_parser_client - INFO - 작업 ID '3d541e4d-21ba-4b1c-9c69-d4c82d346422'의 현재 상태: completed\n",
      "2025-03-10 03:54:01,787 - teddynote_parser_client - INFO - 작업 ID '3d541e4d-21ba-4b1c-9c69-d4c82d346422'가 completed 상태로 완료되었습니다.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "작업 최종 정보:\n",
      "📝 파일명: 2210.03629v3.pdf\n",
      "📝 생성 시간: 2025-03-10 03:53:51\n",
      "📝 완료 시간: 2025-03-10 03:53:59\n",
      "📝 처리 시간: 8.51초\n",
      "📝 ZIP 파일: result/3d541e4d-21ba-4b1c-9c69-d4c82d346422_20250310_035359.zip\n"
     ]
    }
   ],
   "source": [
    "# 작업 상태 확인 및 완료 대기\n",
    "if \"job_id\" in locals() and job_id:\n",
    "    print(f\"작업 ID: {job_id}에 대한 상태를 확인하고 완료될 때까지 대기합니다...\")\n",
    "\n",
    "    try:\n",
    "        # 비동기 작업 완료 대기 (최대 60초, 2초 간격으로 확인)\n",
    "        final_status = client.wait_for_job_completion(\n",
    "            job_id, check_interval=2, max_attempts=30\n",
    "        )\n",
    "\n",
    "        if final_status[\"status\"] == \"completed\":\n",
    "            print(\"\\n작업 최종 정보:\")\n",
    "            print(f\"📝 파일명: {final_status['filename']}\")\n",
    "            print(\n",
    "                f\"📝 생성 시간: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(final_status['created_at']))}\"\n",
    "            )\n",
    "            print(\n",
    "                f\"📝 완료 시간: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(final_status['completed_at']))}\"\n",
    "            )\n",
    "            print(\n",
    "                f\"📝 처리 시간: {final_status['completed_at'] - final_status['created_at']:.2f}초\"\n",
    "            )\n",
    "            print(f\"📝 ZIP 파일: {final_status['zip_filename']}\")\n",
    "        elif final_status[\"status\"] == \"failed\":\n",
    "            print(f\"❌ 작업 실패: {final_status.get('error', '알 수 없는 오류')}\")\n",
    "    except TimeoutError as e:\n",
    "        print(f\"⚠️ 작업 대기 시간 초과: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 작업 상태 확인 중 오류 발생: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 파싱 결과 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-10 03:54:05,517 - teddynote_parser_client - INFO - 작업 ID '3d541e4d-21ba-4b1c-9c69-d4c82d346422'의 현재 상태: completed\n",
      "2025-03-10 03:54:05,518 - teddynote_parser_client - INFO - 작업 ID '3d541e4d-21ba-4b1c-9c69-d4c82d346422'의 결과 다운로드 중...\n",
      "2025-03-10 03:54:05,529 - teddynote_parser_client - INFO - 결과가 성공적으로 다운로드되었습니다: client_test_results/3d541e4d-21ba-4b1c-9c69-d4c82d346422_20250310035405.zip\n",
      "2025-03-10 03:54:05,533 - teddynote_parser_client - INFO - ZIP 파일 'client_test_results/3d541e4d-21ba-4b1c-9c69-d4c82d346422_20250310035405.zip'의 압축을 'client_test_results/3d541e4d-21ba-4b1c-9c69-d4c82d346422'에 해제했습니다.\n",
      "2025-03-10 03:54:05,534 - teddynote_parser_client - INFO - ZIP 파일의 압축이 성공적으로 해제되었습니다: client_test_results/3d541e4d-21ba-4b1c-9c69-d4c82d346422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 파싱 결과가 성공적으로 다운로드되었습니다: client_test_results/3d541e4d-21ba-4b1c-9c69-d4c82d346422_20250310035405.zip\n",
      "✅ ZIP 파일이 성공적으로 압축 해제되었습니다: client_test_results/3d541e4d-21ba-4b1c-9c69-d4c82d346422\n",
      "\n",
      "파싱 결과 디렉토리: client_test_results/3d541e4d-21ba-4b1c-9c69-d4c82d346422\n"
     ]
    }
   ],
   "source": [
    "# 파싱 결과 다운로드 및 압축 해제\n",
    "if (\n",
    "    \"job_id\" in locals()\n",
    "    and job_id\n",
    "    and \"final_status\" in locals()\n",
    "    and final_status[\"status\"] == \"completed\"\n",
    "):\n",
    "    try:\n",
    "        # 결과 다운로드 및 압축 해제\n",
    "        zip_path, extract_path = client.download_result(\n",
    "            job_id=job_id, save_dir=RESULTS_DIR, extract=True, overwrite=True\n",
    "        )\n",
    "\n",
    "        print(f\"✅ 파싱 결과가 성공적으로 다운로드되었습니다: {zip_path}\")\n",
    "        print(f\"✅ ZIP 파일이 성공적으로 압축 해제되었습니다: {extract_path}\")\n",
    "        print(f\"\\n파싱 결과 디렉토리: {extract_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 결과 다운로드 중 오류 발생: {e}\")\n",
    "        extract_path = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 작업 목록 조회"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-10 03:54:17,673 - teddynote_parser_client - INFO - 모든 작업 목록 조회 중...\n",
      "2025-03-10 03:54:17,678 - teddynote_parser_client - INFO - 총 1 개의 작업이 조회되었습니다.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📋 총 1개의 작업이 있습니다.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job ID</th>\n",
       "      <th>Status</th>\n",
       "      <th>Filename</th>\n",
       "      <th>Created At</th>\n",
       "      <th>Completed At</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3d541e4d-21ba-4b1c-9c69-d4c82d346422</td>\n",
       "      <td>completed</td>\n",
       "      <td>2210.03629v3.pdf</td>\n",
       "      <td>2025-03-10 03:53:51</td>\n",
       "      <td>2025-03-10 03:53:59</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Job ID     Status          Filename  \\\n",
       "0  3d541e4d-21ba-4b1c-9c69-d4c82d346422  completed  2210.03629v3.pdf   \n",
       "\n",
       "            Created At         Completed At  \n",
       "0  2025-03-10 03:53:51  2025-03-10 03:53:59  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 작업 목록 조회\n",
    "if api_available:\n",
    "    try:\n",
    "        jobs = client.list_all_jobs()[\"jobs\"]\n",
    "\n",
    "        print(f\"📋 총 {len(jobs)}개의 작업이 있습니다.\\n\")\n",
    "\n",
    "        # 작업 정보를 DataFrame으로 변환\n",
    "        if jobs:\n",
    "            job_data = []\n",
    "            for job in jobs:\n",
    "                created_at = time.strftime(\n",
    "                    \"%Y-%m-%d %H:%M:%S\", time.localtime(job[\"created_at\"])\n",
    "                )\n",
    "                completed_at = (\n",
    "                    time.strftime(\n",
    "                        \"%Y-%m-%d %H:%M:%S\", time.localtime(job[\"completed_at\"])\n",
    "                    )\n",
    "                    if job[\"completed_at\"]\n",
    "                    else \"N/A\"\n",
    "                )\n",
    "\n",
    "                job_data.append(\n",
    "                    {\n",
    "                        \"Job ID\": job[\"job_id\"],\n",
    "                        \"Status\": job[\"status\"],\n",
    "                        \"Filename\": job[\"filename\"],\n",
    "                        \"Created At\": created_at,\n",
    "                        \"Completed At\": completed_at,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            df = pd.DataFrame(job_data)\n",
    "            display(df)\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 작업 목록 조회 중 오류 발생: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "def load_documents_from_pkl(filepath):\n",
    "    \"\"\"\n",
    "    Pickle 파일에서 Langchain Document 리스트를 불러오는 함수\n",
    "\n",
    "    Args:\n",
    "        filepath: 원본 파일 경로 (예: path/to/filename.pdf)\n",
    "    Returns:\n",
    "        Langchain Document 객체 리스트\n",
    "    \"\"\"\n",
    "    # 확장자 제거하고 절대 경로로 변환\n",
    "    abs_path = os.path.abspath(filepath)\n",
    "    base_path = os.path.splitext(abs_path)[0]\n",
    "    pkl_path = f\"{base_path}.pkl\"\n",
    "\n",
    "    with open(pkl_path, \"rb\") as f:\n",
    "        documents = pickle.load(f)\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📄 client_test_results/3d541e4d-21ba-4b1c-9c69-d4c82d346422/3d541e4d-21ba-4b1c-9c69-d4c82d346422/3d541e4d-21ba-4b1c-9c69-d4c82d346422_2210.03629v3.pkl 파일 로드 중...\n",
      "✅ 총 25개의 문서가 로드되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "# extract_path 디렉토리에서 모든 .pkl 파일 찾기\n",
    "pkl_files = glob.glob(str(Path(extract_path) / \"*\" / \"*.pkl\"))\n",
    "\n",
    "if not pkl_files:\n",
    "    print(\"❌ extract_path에서 .pkl 파일을 찾을 수 없습니다.\")\n",
    "else:\n",
    "    # 모든 .pkl 파일에서 문서 로드\n",
    "    all_documents = []\n",
    "    for pkl_file in pkl_files:\n",
    "        print(f\"📄 {pkl_file} 파일 로드 중...\")  # 한국어 코멘트\n",
    "        documents = load_documents_from_pkl(pkl_file)\n",
    "        all_documents.extend(documents)\n",
    "    \n",
    "    print(f\"✅ 총 {len(all_documents)}개의 문서가 로드되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'page': 0, 'source': 'uploads/3d541e4d-21ba-4b1c-9c69-d4c82d346422_2210.03629v3.pdf'}, page_content='# REAC T: SYNERGIZING REASONING AND ACTING IN\\nLANGUAGE MODELS\\nShunyu Yao∗*,1, Jeffrey Zhao2, Dian Yu2, Nan Du2, Izhak Shafran2, Karthik Narasimhan1, Yuan Cao2\\n1Department of Computer Science, Princeton University\\n2Google Research, Brain team\\n1{shunyuy,karthikn}@princeton.edu\\n2{jeffreyzhao,dianyu,dunan,izhak,yuancao}@google.com\\nABSTRACT\\nWhile large language models (LLMs) have demonstrated impressive performance\\nacross tasks in language understanding and interactive decision making, their\\nabilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action\\nplan generation) have primarily been studied as separate topics. In this paper, we\\nexplore the use of LLMs to generate both reasoning traces and task-speciﬁc actions\\nin an interleaved manner, allowing for greater synergy between the two: reasoning\\ntraces help the model induce, track, and update action plans as well as handle\\nexceptions, while actions allow it to interface with and gather additional information'),\n",
       " Document(metadata={'page': 0, 'source': 'uploads/3d541e4d-21ba-4b1c-9c69-d4c82d346422_2210.03629v3.pdf'}, page_content='from external sources such as knowledge bases or environments. We apply our\\napproach, named ReAct, to a diverse set of language and decision making tasks\\nand demonstrate its effectiveness over state-of-the-art baselines in addition to\\nimproved human interpretability and trustworthiness. Concretely, on question\\nanswering (HotpotQA) and fact veriﬁcation (Fever), ReAct overcomes prevalent\\nissues of hallucination and error propagation in chain-of-thought reasoning by\\ninteracting with a simple Wikipedia API, and generating human-like task-solving\\ntrajectories that are more interpretable than baselines without reasoning traces.\\nFurthermore, on two interactive decision making benchmarks (ALFWorld and\\nWebShop), ReAct outperforms imitation and reinforcement learning methods by\\nan absolute success rate of 34% and 10% respectively, while being prompted with\\nonly one or two in-context examples.\\n# 1 INTRODUCTION'),\n",
       " Document(metadata={'page': 0, 'source': 'uploads/3d541e4d-21ba-4b1c-9c69-d4c82d346422_2210.03629v3.pdf'}, page_content='A unique feature of human intelligence is the ability to seamlessly combine task-oriented actions with\\nverbal reasoning (or inner speech, Alderson-Day & Fernyhough, 2015), which has been theorized to\\nplay an important role in human cognition for enabling self-regulation or strategization (Vygotsky,\\n1987; Luria, 1965; Fernyhough, 2010) and maintaining a working memory (Baddeley, 1992). Con-\\nsider the example of cooking up a dish in the kitchen. Between any two speciﬁc actions, we may\\nreason in language in order to track progress (“now that everything is cut, I should heat up the pot of\\nwater”), to handle exceptions or adjust the plan according to the situation (“I don’t have salt, so let\\nme use soy sauce and pepper instead”), and to realize when external information is needed (“how do\\nI prepare dough? Let me search on the Internet”). We may also act (open a cookbook to read the\\nrecipe, open the fridge, check ingredients) to support the reasoning and to answer questions (“What'),\n",
       " Document(metadata={'page': 0, 'source': 'uploads/3d541e4d-21ba-4b1c-9c69-d4c82d346422_2210.03629v3.pdf'}, page_content='dish can I make right now?”). This tight synergy between “acting” and “reasoning” allows humans\\nto learn new tasks quickly and perform robust decision making or reasoning, even under previously\\nunseen circumstances or facing information uncertainties.\\nRecent results have hinted at the possibility of combining verbal reasoning with interactive decision\\nmaking in autonomous systems. On one hand, properly prompted large language models (LLMs)\\nhave demonstrated emergent capabilities to carry out several steps of reasoning traces to derive'),\n",
       " Document(metadata={'page': 1, 'source': 'uploads/3d541e4d-21ba-4b1c-9c69-d4c82d346422_2210.03629v3.pdf'}, page_content='Figure 1: (1) Comparison of 4 prompting methods, (a) Standard, (b) Chain-of-thought (CoT,\\nReason Only), (c) Act-only, and (d) ReAct (Reason+Act), solving a HotpotQA (Yang et al., 2018)\\nquestion. (2) Comparison of (a) Act-only and (b) ReAct prompting to solve an AlfWorld (Shridhar\\net al., 2020b) game. In both domains, we omit in-context examples in the prompt, and only show task\\nsolving trajectories generated by the model (Act, Thought) and the environment (Obs).\\nanswers from questions in arithmetic, commonsense, and symbolic reasoning tasks (Wei et al.,\\n2022). However, this “chain-of-thought” reasoning is a static black box, in that the model uses\\nits own internal representations to generate thoughts and is not grounded in the external world,\\nwhich limits its ability to reason reactively or update its knowledge. This can lead to issues like fact\\nhallucination and error propagation over the reasoning process (Figure 1 (1b)). On the other hand,'),\n",
       " Document(metadata={'page': 1, 'source': 'uploads/3d541e4d-21ba-4b1c-9c69-d4c82d346422_2210.03629v3.pdf'}, page_content='recent work has explored the use of pre-trained language models for planning and acting in interactive\\nenvironments (Ahn et al., 2022; Nakano et al., 2021; Yao et al., 2020; Huang et al., 2022a), with\\na focus on predicting actions via language priors. These approaches usually convert multi-modal\\nobservations into text, use a language model to generate domain-speciﬁc actions or plans, and then\\nuse a controller to choose or execute them. However, they do not employ language models to reason\\nabstractly about high-level goals or maintain a working memory to support acting, barring Huang\\net al. (2022b) who perform a limited form of verbal reasoning to reiterate spatial facts about the\\ncurrent state. Beyond such simple embodied tasks to interact with a few blocks, there have not been\\nstudies on how reasoning and acting can be combined in a synergistic manner for general task solving,\\nand if such a combination can bring systematic beneﬁts compared to reasoning or acting alone.'),\n",
       " Document(metadata={'page': 1, 'source': 'uploads/3d541e4d-21ba-4b1c-9c69-d4c82d346422_2210.03629v3.pdf'}, page_content='In this work, we present ReAct, a general paradigm to combine reasoning and acting with language\\nmodels for solving diverse language reasoning and decision making tasks (Figure 1). ReAct\\nprompts LLMs to generate both verbal reasoning traces and actions pertaining to a task in an\\ninterleaved manner, which allows the model to perform dynamic reasoning to create, maintain, and\\nadjust high-level plans for acting (reason to act), while also interact with the external environments\\n(e.g. Wikipedia) to incorporate additional information into reasoning (act to reason).'),\n",
       " Document(metadata={'image': '![](file:////app/uploads/3d541e4d-21ba-4b1c-9c69-d4c82d346422_2210.03629v3_Page_1_Index_13.png)', 'entity': '<image>\\n<title>\\nComparative Analysis of Prompting Methods in AI Reasoning\\n</title>\\n<details>\\nThe image illustrates a comparison of four prompting methods used in AI reasoning tasks: Standard, Chain-of-Thought (CoT), Act-Only, and ReAct (Reason + Act). It highlights how these methods approach problem-solving in different scenarios, specifically in HotpotQA and AlfWorld tasks. The ReAct method is emphasized for its ability to interleave reasoning and acting, allowing for dynamic interaction with the environment.\\n</details>\\n<entities>\\nAI prompting methods, HotpotQA, AlfWorld, Apple Remote, Front Row software\\n</entities>\\n<hypothetical_questions>\\n- How might the integration of reasoning and acting improve AI performance in complex tasks?\\n- What challenges could arise from using the ReAct method in real-world applications?\\n</hypothetical_questions>\\n</image>', 'page': 1, 'source': 'uploads/3d541e4d-21ba-4b1c-9c69-d4c82d346422_2210.03629v3.pdf'}, page_content='![](file:////app/uploads/3d541e4d-21ba-4b1c-9c69-d4c82d346422_2210.03629v3_Page_1_Index_13.png)\\n\\n<image>\\n<title>\\nComparative Analysis of Prompting Methods in AI Reasoning\\n</title>\\n<details>\\nThe image illustrates a comparison of four prompting methods used in AI reasoning tasks: Standard, Chain-of-Thought (CoT), Act-Only, and ReAct (Reason + Act). It highlights how these methods approach problem-solving in different scenarios, specifically in HotpotQA and AlfWorld tasks. The ReAct method is emphasized for its ability to interleave reasoning and acting, allowing for dynamic interaction with the environment.\\n</details>\\n<entities>\\nAI prompting methods, HotpotQA, AlfWorld, Apple Remote, Front Row software\\n</entities>\\n<hypothetical_questions>\\n- How might the integration of reasoning and acting improve AI performance in complex tasks?\\n- What challenges could arise from using the ReAct method in real-world applications?\\n</hypothetical_questions>\\n</image>'),\n",
       " Document(metadata={'page': 2, 'source': 'uploads/3d541e4d-21ba-4b1c-9c69-d4c82d346422_2210.03629v3.pdf'}, page_content='We conduct empirical evaluations of ReAct and state-of-the-art baselines on four diverse benchmarks:\\nquestion answering (HotPotQA, Yang et al., 2018), fact veriﬁcation (Fever, Thorne et al., 2018),\\ntext-based game (ALFWorld, Shridhar et al., 2020b), and webpage navigation (WebShop, Yao\\net al., 2022). For HotPotQA and Fever, with access to a Wikipedia API that the model can interact\\nwith, ReAct outperforms vanilla action generation models while being competitive with chain-of-\\nthought reasoning (CoT) (Wei et al., 2022). The best approach overall is a combination of ReAct\\nand CoT that allows for the use of both internal knowledge and externally obtained information\\nduring reasoning. On ALFWorld and WebShop, two or even one-shot ReAct prompting is able\\nto outperform imitation or reinforcement learning methods trained with 103 ∼ 105 task instances,\\nwith an absolute improvement of 34% and 10% in success rates respectively. We also demonstrate'),\n",
       " Document(metadata={'page': 2, 'source': 'uploads/3d541e4d-21ba-4b1c-9c69-d4c82d346422_2210.03629v3.pdf'}, page_content='the importance of sparse, versatile reasoning in decision making by showing consistent advantages\\nover controlled baselines with actions only. Besides general applicability and performance boost,\\nthe combination of reasoning and acting also contributes to model interpretability, trustworthiness,\\nand diagnosability across all domains, as humans can readily distinguish information from model’s\\ninternal knowledge versus external environments, as well as inspect reasoning traces to understand\\nthe decision basis of model actions.\\nTo summarize, our key contributions are the following: (1) we introduce ReAct, a novel prompt-\\nbased paradigm to synergize reasoning and acting in language models for general task solving; (2) we\\nperform extensive experiments across diverse benchmarks to showcase the advantage of ReAct in a\\nfew-shot learning setup over prior approaches that perform either reasoning or action generation in'),\n",
       " Document(metadata={'page': 2, 'source': 'uploads/3d541e4d-21ba-4b1c-9c69-d4c82d346422_2210.03629v3.pdf'}, page_content='isolation; (3) we present systematic ablations and analysis to understand the importance of acting in\\nreasoning tasks, and reasoning in interactive tasks; (4) we analyze the limitations of ReAct under the\\nprompting setup (i.e. limited support of reasoning and acting behaviors), and perform initial ﬁnetuning\\nexperiments showing the potential of ReAct to improve with additional training data. Scaling up\\nReAct to train and operate on more tasks and combining it with complementary paradigms like\\nreinforcement learning could further unlock the potential of large language models.\\n2 REAC T: SYNERGIZING RE ASONING + AC T ING\\nConsider a general setup of an agent interacting with an environment for task solving. At time\\nstep t, an agent receives an observation ot ∈ O from the environment and takes an action at ∈ A\\nfollowing some policy π(at|ct), where ct = (o1, a1, · · · , ot−1, at−1, ot) is the context to the agent.'),\n",
       " Document(metadata={'page': 2, 'source': 'uploads/3d541e4d-21ba-4b1c-9c69-d4c82d346422_2210.03629v3.pdf'}, page_content='Learning a policy is challenging when the mapping ct (cid:55)→ at is highly implicit and requires extensive\\ncomputation. For example, the agent shown in Figure 1(1c) is unable to generate the correct ﬁnal\\naction (Act 4) to ﬁnish the QA task as it requires complex reasoning over the trajectory context\\n(Question, Act 1-3, Obs 1-3). Similarly, the agent shown in Figure 1(2a) fails to comprehend from the\\ncontext that sinkbasin 1 does not contain peppershaker 1, thus keep producing hallucinating actions.\\nˆ = A ∪ L, where L is the\\nThe idea of ReAct is simple: we augment the agent’s action space to A\\nspace of language. An action ˆat ∈ L in the language space, which we will refer to as a thought or a\\nreasoning trace, does not affect the external environment, thus leading to no observation feedback.\\nInstead, a thought ˆat aims to compose useful information by reasoning over the current context ct,'),\n",
       " Document(metadata={'page': 2, 'source': 'uploads/3d541e4d-21ba-4b1c-9c69-d4c82d346422_2210.03629v3.pdf'}, page_content='and update the context ct+1 = (ct, ˆat) to support future reasoning or acting. As shown in Figure 1,\\nthere could be various types of useful thoughts, e.g. decomposing task goals and create action plans\\n(2b, Act 1; 1d, Thought 1), injecting commonsense knowledge relevant to task solving (2b, Act 1),\\nextracting important parts from observations (1d, Thought2, 4), track progress and transit action plans\\n(2b, Act 8), handle exceptions and adjust action plans (1d, Thought 3), and so on.\\nHowever, as the language space L is unlimited, learning in this augmented action space is difﬁcult\\nand requires strong language priors. In this paper, we mainly focus on the setup where a frozen\\nlarge language model, PaLM-540B (Chowdhery et al., 2022)1, is prompted with few-shot in-context\\nexamples to generate both domain-speciﬁc actions and free-form language thoughts for task solving\\n(Figure 1 (1d), (2b)). Each in-context example is a human trajectory of actions, thoughts, and'),\n",
       " Document(metadata={'page': 2, 'source': 'uploads/3d541e4d-21ba-4b1c-9c69-d4c82d346422_2210.03629v3.pdf'}, page_content='environment observations to solve a task instance (see Appendix C). For the tasks where reasoning is\\nof primary importance (Figure 1(1)), we alternate the generation of thoughts and actions so that the\\ntask-solving trajectory consists of multiple thought-action-observation steps. In contrast, for decision\\nmaking tasks that potentially involve a large number of actions (Figure 1(2)), thoughts only need to'),\n",
       " Document(metadata={'page': 3, 'source': 'uploads/3d541e4d-21ba-4b1c-9c69-d4c82d346422_2210.03629v3.pdf'}, page_content='appear sparsely in the most relevant positions of a trajectory, so we let the language model decide the\\nasynchronous occurrence of thoughts and actions for itself.\\nSince decision making and reasoning capabilities are integrated into a large language model, ReAct\\nenjoys several unique features: A) Intuitive and easy to design: Designing ReAct prompts is\\nstraightforward as human annotators just type down their thoughts in language on top of their actions\\ntaken. No ad-hoc format choice, thought design, or example selection is used in this paper. We detail\\nprompt design for each task in Sections 3 and 4. B) General and ﬂexible: Due to the ﬂexible thought\\nspace and thought-action occurrence format, ReAct works for diverse tasks with distinct action\\nspaces and reasoning needs, including but not limited to QA, fact veriﬁcation, text game, and web\\nnavigation. C) Performant and robust: ReAct shows strong generalization to new task instances'),\n",
       " Document(metadata={'page': 3, 'source': 'uploads/3d541e4d-21ba-4b1c-9c69-d4c82d346422_2210.03629v3.pdf'}, page_content='while learning solely from one to six in-context examples, consistently outperforming baselines with\\nonly reasoning or acting across different domains. We also show in Section 3 additional beneﬁts\\nwhen ﬁnetuning is enabled, and in Section 4 how ReAct performance is robust to prompt selections.\\nD) Human aligned and controllable: ReAct promises an interpretable sequential decision making\\nand reasoning process where humans can easily inspect reasoning and factual correctness. Moreover,\\nhumans can also control or correct the agent behavior on the go by thought editing, as shown in\\nFigure 5 in Section 4.\\n# 3 KNOWLEDGE-INTENSIVE REASONING TASKS\\nWe begin with knowledge-intensive reasoning tasks like multi-hop question answering and fact\\nveriﬁcation. As shown in Figure 1(1d), by interacting with a Wikipedia API, ReAct is able to\\nretrieve information to support reasoning, while also use reasoning to target what to retrieve next,\\ndemonstrating a synergy of reasoning and acting.\\n3.1 SETUP'),\n",
       " Document(metadata={'page': 3, 'source': 'uploads/3d541e4d-21ba-4b1c-9c69-d4c82d346422_2210.03629v3.pdf'}, page_content='Domains We consider two datasets challenging knowledge retrieval and reasoning: (1) Hot-\\nPotQA (Yang et al., 2018), a multi-hop question answering benchmark that requires reasoning\\nover two or more Wikipedia passages, and (2) FEVER (Thorne et al., 2018), a fact veriﬁcation\\nbenchmark where each claim is annotated SUPPORTS, REFUTES, or NOT ENOUGH INFO, based\\non if there exists a Wikipedia passage to verify the claim. In this work, we operate in a question-only\\nsetup for both tasks, where models only receive the question/claim as input without access to support\\nparagraphs, and have to rely on their internal knowledge or retrieve knowledge via interacting with\\nan external environment to support reasoning.\\nAction Space We design a simple Wikipedia web API with three types of actions to support\\ninteractive information retrieval: (1) search[entity], which returns the ﬁrst 5 sentences from\\nthe corresponding entity wiki page if it exists, or else suggests top-5 similar entities from the'),\n",
       " Document(metadata={'page': 3, 'source': 'uploads/3d541e4d-21ba-4b1c-9c69-d4c82d346422_2210.03629v3.pdf'}, page_content='Wikipedia search engine, (2) lookup[string], which would return the next sentence in the page\\ncontaining string, simulating Ctrl+F functionality on the browser. (3) finish[answer], which\\nwould ﬁnish the current task with answer. We note that this action space mostly can only retrieve a\\nsmall part of a passage based on exact passage name, which is signiﬁcantly weaker than state-of-the-\\nart lexical or neural retrievers. The purpose is to simulate how humans would interact with Wikipedia,\\nand force models to retrieve via explicit reasoning in language.\\n3.2 METHODS\\nReAct Prompting For HotpotQA and Fever, we randomly select 6 and 3 cases2 from the training\\nset and manually compose ReAct-format trajectories to use as few-shot exemplars in the prompts.\\nSimilar to Figure 1(d), each trajectory consists of multiple thought-action-observation steps (i.e. dense\\nthought), where free-form thoughts are used for various purposes. Speciﬁcally, we use a combination'),\n",
       " Document(metadata={'page': 3, 'source': 'uploads/3d541e4d-21ba-4b1c-9c69-d4c82d346422_2210.03629v3.pdf'}, page_content='of thoughts that decompose questions (“I need to search x, ﬁnd y, then ﬁnd z”), extract information\\nfrom Wikipedia observations (“x was started in 1844”, “The paragraph does not tell x”), perform\\ncommonsense (“x is not y, so z must instead be...”) or arithmetic reasoning (“1844 < 1989”), guide'),\n",
       " Document(metadata={'page': 4, 'source': 'uploads/3d541e4d-21ba-4b1c-9c69-d4c82d346422_2210.03629v3.pdf'}, page_content='Table 1: PaLM-540B prompting results on\\nHotpotQA and Fever.\\nFigure 2: PaLM-540B prompting results with respect to\\nnumber of CoT-SC samples used.\\nsearch reformulation (“maybe I can search/look up x instead”), and synthesize the ﬁnal answer (“...so\\nthe answer is x”). See Appendix C for more details.\\nBaselines We systematically ablate ReAct trajectories to build prompts for multiple baselines (with\\nformats as Figure 1(1a-1c)): (a) Standard prompting (Standard), which removes all thoughts,\\nactions, observations in ReAct trajectories. (b) Chain-of-thought prompting (CoT) (Wei et al.,\\n2022), which removes actions and observations and serve as a reasoning-only baseline. We also\\nbuild a self-consistency baseline (CoT-SC) (Wang et al., 2022a;b) by sampling 21 CoT trajectories\\nwith decoding temperature 0.7 during inference and adopting the majority answer, which is found to\\nconsistently boost performance over CoT. (c) Acting-only prompt (Act), which removes thoughts'),\n",
       " Document(metadata={'page': 4, 'source': 'uploads/3d541e4d-21ba-4b1c-9c69-d4c82d346422_2210.03629v3.pdf'}, page_content='in ReAct trajectories, loosely resembling how WebGPT (Nakano et al., 2021) interacts with the\\nInternet to answer questions, though it operates on a different task and action space, and uses imitation\\nand reinforcement learning instead of prompting.\\nCombining Internal and External Knowledge As will be detail in Section 3.3, we observe that\\nthe problem solving process demonstrated by ReAct is more factual and grounded, whereas CoT\\nis more accurate in formulating reasoning structure but can easily suffer from hallucinated facts\\nor thoughts. We therefore propose to incorporate ReAct and CoT-SC, and let the model decide\\nwhen to switch to the other method based on the following heuristics: A) ReAct → CoT-SC: when\\nReAct fails to return an answer within given steps, back off to CoT-SC. We set 7 and 5 steps for\\nHotpotQA and FEVER respectively as we ﬁnd more steps will not improve ReAct performance3.\\nB) CoT-SC → ReAct: when the majority answer among n CoT-SC samples occurs less than n/2'),\n",
       " Document(metadata={'page': 4, 'source': 'uploads/3d541e4d-21ba-4b1c-9c69-d4c82d346422_2210.03629v3.pdf'}, page_content='times (i.e. internal knowledge might not support the task conﬁdently), back off to ReAct.\\nFinetuning Due to the challenge of manually annotating reasoning traces and actions at scale,\\nwe consider a bootstraping approach similar to Zelikman et al. (2022), using 3,000 trajectories\\nwith correct answers generated by ReAct (also for other baselines) to ﬁnetune smaller language\\nmodels (PaLM-8/62B) to decode trajectories (all thoughts, actions, observations) conditioned on\\ninput questions/claims. More details are in Appendix B.1.\\n3.3 RESULTS AND OBSERVATIONS\\nReAct outperforms Act consistently Table 1 shows HotpotQA and Fever results using PaLM-\\n540B as the base model with different prompting methods. We note that ReAct is better than Act\\non both tasks, demonstrating the value of reasoning to guide acting, especially for synthesizing the\\nﬁnal answer, as shown in Figure 1 (1c-d). Fine-tuning results 3 also conﬁrm the beneﬁt of reasoning\\ntraces for more informed acting.'),\n",
       " Document(metadata={'image': '![](file:////app/uploads/3d541e4d-21ba-4b1c-9c69-d4c82d346422_2210.03629v3_Page_4_Index_41.png)', 'entity': \"<image>\\n<title>\\nPerformance Comparison of Prompting Methods in AI Models\\n</title>\\n<details>\\nThe image presents two graphs illustrating the performance of various prompting methods (CoT-SC, ReAct, and CoT) on the HotpotQA and Fever datasets. The left graph shows the accuracy of HotpotQA as the number of CoT-SC trials increases, while the right graph displays the accuracy for the Fever dataset. The lines represent different methods, indicating how each method's performance evolves with more trials.\\n</details>\\n<entities>\\nHotpotQA, Fever, CoT-SC, ReAct, CoT, AI models\\n</entities>\\n<hypothetical_questions>\\n- How might the performance of these prompting methods change with further trials beyond 20?\\n- What factors could influence the effectiveness of each prompting method in different contexts?\\n</hypothetical_questions>\\n</image>\", 'page': 4, 'source': 'uploads/3d541e4d-21ba-4b1c-9c69-d4c82d346422_2210.03629v3.pdf'}, page_content=\"![](file:////app/uploads/3d541e4d-21ba-4b1c-9c69-d4c82d346422_2210.03629v3_Page_4_Index_41.png)\\n\\n<image>\\n<title>\\nPerformance Comparison of Prompting Methods in AI Models\\n</title>\\n<details>\\nThe image presents two graphs illustrating the performance of various prompting methods (CoT-SC, ReAct, and CoT) on the HotpotQA and Fever datasets. The left graph shows the accuracy of HotpotQA as the number of CoT-SC trials increases, while the right graph displays the accuracy for the Fever dataset. The lines represent different methods, indicating how each method's performance evolves with more trials.\\n</details>\\n<entities>\\nHotpotQA, Fever, CoT-SC, ReAct, CoT, AI models\\n</entities>\\n<hypothetical_questions>\\n- How might the performance of these prompting methods change with further trials beyond 20?\\n- What factors could influence the effectiveness of each prompting method in different contexts?\\n</hypothetical_questions>\\n</image>\"),\n",
       " Document(metadata={'table': '| Prompt Methoda | HotpotQA (EM) | Fever (Acc) |\\n| --- | --- | --- |\\n| Standard | 28.7 | 57.1 |\\n| CoT (Wei et al., 2022) | 29.4 | 56.3 |\\n| CoT-SC (Wang et al., 2022a) | 33.4 | 60.4 |\\n| Act | 25.7 | 58.9 |\\n| ReAct | 27.4 | 60.9 |\\n| CoT-SC → ReAct | 34.2 | 64.6 |\\n| ReAct→ CoT-SC | 35.1 | 62.0 |\\n| Supervised SoTAb | 67.5 | 89.5 |\\n\\n', 'entity': '', 'page': 4, 'source': 'uploads/3d541e4d-21ba-4b1c-9c69-d4c82d346422_2210.03629v3.pdf'}, page_content='| Prompt Methoda | HotpotQA (EM) | Fever (Acc) |\\n| --- | --- | --- |\\n| Standard | 28.7 | 57.1 |\\n| CoT (Wei et al., 2022) | 29.4 | 56.3 |\\n| CoT-SC (Wang et al., 2022a) | 33.4 | 60.4 |\\n| Act | 25.7 | 58.9 |\\n| ReAct | 27.4 | 60.9 |\\n| CoT-SC → ReAct | 34.2 | 64.6 |\\n| ReAct→ CoT-SC | 35.1 | 62.0 |\\n| Supervised SoTAb | 67.5 | 89.5 |\\n\\n\\n\\n'),\n",
       " Document(metadata={'table': '| Prompt Methoda | HotpotQA (EM) | Fever (Acc) |\\n| --- | --- | --- |\\n| Standard | 28.7 | 57.1 |\\n| CoT (Wei et al., 2022) | 29.4 | 56.3 |\\n| CoT-SC (Wang et al., 2022a) | 33.4 | 60.4 |\\n| Act | 25.7 | 58.9 |\\n| ReAct | 27.4 | 60.9 |\\n| CoT-SC → ReAct | 34.2 | 64.6 |\\n| ReAct→ CoT-SC | 35.1 | 62.0 |\\n| Supervised SoTAb | 67.5 | 89.5 |\\n\\n', 'entity': '', 'page': 4, 'source': 'uploads/3d541e4d-21ba-4b1c-9c69-d4c82d346422_2210.03629v3.pdf'}, page_content='')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_documents"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
