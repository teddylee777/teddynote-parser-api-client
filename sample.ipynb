{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TeddyNote Parser API í´ë¼ì´ì–¸íŠ¸ í…ŒìŠ¤íŠ¸\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì€ `parser_client.py`ì— êµ¬í˜„ëœ `TeddyNoteParserClient` í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ TeddyNote Parser APIì™€ ìƒí˜¸ì‘ìš©í•˜ëŠ” ê³¼ì •ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.\n",
    "\n",
    "ì£¼ìš” ë‹¨ê³„:\n",
    "1. í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ë° API ì—°ê²° í™•ì¸\n",
    "2. PDF íŒŒì¼ ì—…ë¡œë“œ ë° íŒŒì‹± ì‘ì—… ìš”ì²­\n",
    "3. ì‘ì—… ìƒíƒœ í™•ì¸\n",
    "4. ì™„ë£Œëœ íŒŒì‹± ê²°ê³¼ ë‹¤ìš´ë¡œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -U teddynote-parser-api pandas langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## í™˜ê²½ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# API ì„œë²„ ì„¤ì •\n",
    "API_URL = \"http://localhost:9990\"  # API ì„œë²„ ì£¼ì†Œ\n",
    "\n",
    "# PDF íŒŒì¼ ê²½ë¡œ\n",
    "PDF_FILE_PATH = \"data/2210.03629v3.pdf\"  # PDF íŒŒì¼ ê²½ë¡œ\n",
    "\n",
    "# ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬\n",
    "OUTPUT_DIR = \"client_test_results\"\n",
    "RESULTS_DIR = Path(OUTPUT_DIR)\n",
    "RESULTS_DIR.mkdir(exist_ok=True)  # ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±\n",
    "\n",
    "# API í‚¤ ì„¤ì • (í™˜ê²½ ë³€ìˆ˜ì—ì„œ ë¡œë“œí•˜ê±°ë‚˜ ì§ì ‘ ì§€ì •)\n",
    "UPSTAGE_API_KEY = os.environ.get(\"UPSTAGE_API_KEY\", \"your_upstage_api_key\")\n",
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\", \"your_openai_api_key\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ë° ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from teddynote_parser_client.client import TeddyNoteParserClient\n",
    "\n",
    "# ë¡œê¹… ì„¤ì •\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "logger = logging.getLogger(\"teddynote_parser_client\")\n",
    "\n",
    "# TeddyNoteParserClient ì¸ìŠ¤í„´ìŠ¤ ìƒì„±\n",
    "client = TeddyNoteParserClient(\n",
    "    api_url=API_URL,  # API ì„œë²„ URL\n",
    "    upstage_api_key=UPSTAGE_API_KEY,  # UPSTAGE LLM API í‚¤\n",
    "    openai_api_key=OPENAI_API_KEY,  # OpenAI API í‚¤ (ëŒ€ì²´ LLMìœ¼ë¡œ ì‚¬ìš©)\n",
    "    batch_size=50,  # í•œ ë²ˆì— ì²˜ë¦¬í•  PDF í˜ì´ì§€ ìˆ˜\n",
    "    test_page=None,  # í…ŒìŠ¤íŠ¸ìš© í˜ì´ì§€ ì œí•œ (None: ì „ì²´ í˜ì´ì§€ ì²˜ë¦¬)\n",
    "    language=\"Korean\",  # ë¬¸ì„œ ì–¸ì–´ ì„¤ì •\n",
    "    include_image=True,  # ê²°ê³¼ì— ì´ë¯¸ì§€ í¬í•¨ ì—¬ë¶€\n",
    "    logger=logger,  # ë¡œê¹…ì— ì‚¬ìš©í•  ë¡œê±°\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì„œë²„ì— ì •ìƒ ë™ì‘ì¤‘ì¸ì§€ í™•ì¸í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-10 03:53:49,860 - teddynote_parser_client - INFO - API ì„œë²„ê°€ ì •ìƒì ìœ¼ë¡œ ì‘ë‹µí–ˆìŠµë‹ˆë‹¤.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… API ì„œë²„ê°€ ì •ìƒì ìœ¼ë¡œ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤. ìƒíƒœ: {'status': 'ok', 'timestamp': '2025-03-10T03:53:49.858499'}\n"
     ]
    }
   ],
   "source": [
    "# API ì„œë²„ ìƒíƒœ í™•ì¸\n",
    "try:\n",
    "    health_status = client.health_check()\n",
    "    print(f\"âœ… API ì„œë²„ê°€ ì •ìƒì ìœ¼ë¡œ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤. ìƒíƒœ: {health_status}\")\n",
    "    api_available = True\n",
    "except Exception as e:\n",
    "    print(f\"âŒ API ì„œë²„ì— ì ‘ì†í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}\")\n",
    "    print(\"API ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.\")\n",
    "    api_available = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. PDF íŒŒì¼ ì—…ë¡œë“œ ë° íŒŒì‹± ì‘ì—… ìš”ì²­"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-10 03:53:51,338 - teddynote_parser_client - INFO - íŒŒì¼ '2210.03629v3.pdf'ì— ëŒ€í•œ íŒŒì‹± ì‘ì—… ìš”ì²­ ì¤‘...\n",
      "2025-03-10 03:53:51,339 - teddynote_parser_client - INFO - íŒŒì‹± ì˜µì…˜: ì–¸ì–´=English, ì´ë¯¸ì§€ í¬í•¨=True, ë°°ì¹˜ í¬ê¸°=50, ì²˜ë¦¬ í˜ì´ì§€ ìˆ˜=5\n",
      "2025-03-10 03:53:51,359 - teddynote_parser_client - INFO - íŒŒì‹± ì‘ì—…ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤. ì‘ì—… ID: 3d541e4d-21ba-4b1c-9c69-d4c82d346422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“„ íŒŒì¼ 'data/2210.03629v3.pdf'ì— ëŒ€í•œ íŒŒì‹± ì‘ì—…ì„ ìš”ì²­í•©ë‹ˆë‹¤...\n",
      "âœ… íŒŒì‹± ì‘ì—…ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤!\n",
      "ğŸ“ ì‘ì—… ID: 3d541e4d-21ba-4b1c-9c69-d4c82d346422\n",
      "ğŸ“ ìƒíƒœ: pending\n",
      "ğŸ“ ë©”ì‹œì§€: PDF íŒŒì‹± ì‘ì—…ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# PDF íŒŒì¼ íŒŒì‹± ìš”ì²­\n",
    "if api_available:\n",
    "    try:\n",
    "        print(f\"ğŸ“„ íŒŒì¼ '{PDF_FILE_PATH}'ì— ëŒ€í•œ íŒŒì‹± ì‘ì—…ì„ ìš”ì²­í•©ë‹ˆë‹¤...\")\n",
    "        # pdf_path: íŒŒì‹±í•  PDF íŒŒì¼ ê²½ë¡œ\n",
    "        # batch_size: í•œ ë²ˆì— ì²˜ë¦¬í•  í˜ì´ì§€ ìˆ˜(ê¸°ë³¸ê°’: 30, ìµœëŒ€ê°’: 100)\n",
    "        # language: Entity íŒŒì‹± ì–¸ì–´\n",
    "        # test_page: í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ì²˜ìŒ ~ ì§€ì •í•œ í˜ì´ì§€ê¹Œì§€ë§Œ ì²˜ë¦¬(ê¸°ë³¸ê°’: None - ëª¨ë“  í˜ì´ì§€ ì²˜ë¦¬)\n",
    "        # include_image: ê²°ê³¼ì— ì´ë¯¸ì§€ í¬í•¨ ì—¬ë¶€(ê¸°ë³¸ê°’: True)\n",
    "        parse_result = client.parse_pdf(\n",
    "            pdf_path=PDF_FILE_PATH,\n",
    "            batch_size=50,\n",
    "            language=\"English\",\n",
    "            test_page=5,\n",
    "            include_image=True,\n",
    "        )\n",
    "\n",
    "        job_id = parse_result[\"job_id\"]\n",
    "        print(f\"âœ… íŒŒì‹± ì‘ì—…ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤!\")\n",
    "        print(f\"ğŸ“ ì‘ì—… ID: {job_id}\")\n",
    "        print(f\"ğŸ“ ìƒíƒœ: {parse_result['status']}\")\n",
    "        print(f\"ğŸ“ ë©”ì‹œì§€: {parse_result['message']}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ íŒŒì‹± ì‘ì—… ìš”ì²­ ì‹¤íŒ¨: {e}\")\n",
    "        job_id = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ì‘ì—… ìƒíƒœ í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-10 03:53:53,731 - teddynote_parser_client - INFO - ì‘ì—… ID '3d541e4d-21ba-4b1c-9c69-d4c82d346422'ì˜ ì™„ë£Œ ëŒ€ê¸° ì¤‘...\n",
      "2025-03-10 03:53:53,731 - teddynote_parser_client - INFO - ìƒíƒœ í™•ì¸ ê°„ê²©: 2ì´ˆ, ìµœëŒ€ ì‹œë„ íšŸìˆ˜: 30íšŒ\n",
      "2025-03-10 03:53:53,736 - teddynote_parser_client - INFO - ì‘ì—… ID '3d541e4d-21ba-4b1c-9c69-d4c82d346422'ì˜ í˜„ì¬ ìƒíƒœ: processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì‘ì—… ID: 3d541e4d-21ba-4b1c-9c69-d4c82d346422ì— ëŒ€í•œ ìƒíƒœë¥¼ í™•ì¸í•˜ê³  ì™„ë£Œë  ë•Œê¹Œì§€ ëŒ€ê¸°í•©ë‹ˆë‹¤...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-10 03:53:55,748 - teddynote_parser_client - INFO - ì‘ì—… ID '3d541e4d-21ba-4b1c-9c69-d4c82d346422'ì˜ í˜„ì¬ ìƒíƒœ: processing\n",
      "2025-03-10 03:53:57,768 - teddynote_parser_client - INFO - ì‘ì—… ID '3d541e4d-21ba-4b1c-9c69-d4c82d346422'ì˜ í˜„ì¬ ìƒíƒœ: processing\n",
      "2025-03-10 03:53:59,776 - teddynote_parser_client - INFO - ì‘ì—… ID '3d541e4d-21ba-4b1c-9c69-d4c82d346422'ì˜ í˜„ì¬ ìƒíƒœ: processing\n",
      "2025-03-10 03:54:01,787 - teddynote_parser_client - INFO - ì‘ì—… ID '3d541e4d-21ba-4b1c-9c69-d4c82d346422'ì˜ í˜„ì¬ ìƒíƒœ: completed\n",
      "2025-03-10 03:54:01,787 - teddynote_parser_client - INFO - ì‘ì—… ID '3d541e4d-21ba-4b1c-9c69-d4c82d346422'ê°€ completed ìƒíƒœë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ì‘ì—… ìµœì¢… ì •ë³´:\n",
      "ğŸ“ íŒŒì¼ëª…: 2210.03629v3.pdf\n",
      "ğŸ“ ìƒì„± ì‹œê°„: 2025-03-10 03:53:51\n",
      "ğŸ“ ì™„ë£Œ ì‹œê°„: 2025-03-10 03:53:59\n",
      "ğŸ“ ì²˜ë¦¬ ì‹œê°„: 8.51ì´ˆ\n",
      "ğŸ“ ZIP íŒŒì¼: result/3d541e4d-21ba-4b1c-9c69-d4c82d346422_20250310_035359.zip\n"
     ]
    }
   ],
   "source": [
    "# ì‘ì—… ìƒíƒœ í™•ì¸ ë° ì™„ë£Œ ëŒ€ê¸°\n",
    "if \"job_id\" in locals() and job_id:\n",
    "    print(f\"ì‘ì—… ID: {job_id}ì— ëŒ€í•œ ìƒíƒœë¥¼ í™•ì¸í•˜ê³  ì™„ë£Œë  ë•Œê¹Œì§€ ëŒ€ê¸°í•©ë‹ˆë‹¤...\")\n",
    "\n",
    "    try:\n",
    "        # ë¹„ë™ê¸° ì‘ì—… ì™„ë£Œ ëŒ€ê¸° (ìµœëŒ€ 60ì´ˆ, 2ì´ˆ ê°„ê²©ìœ¼ë¡œ í™•ì¸)\n",
    "        final_status = client.wait_for_job_completion(\n",
    "            job_id, check_interval=2, max_attempts=30\n",
    "        )\n",
    "\n",
    "        if final_status[\"status\"] == \"completed\":\n",
    "            print(\"\\nì‘ì—… ìµœì¢… ì •ë³´:\")\n",
    "            print(f\"ğŸ“ íŒŒì¼ëª…: {final_status['filename']}\")\n",
    "            print(\n",
    "                f\"ğŸ“ ìƒì„± ì‹œê°„: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(final_status['created_at']))}\"\n",
    "            )\n",
    "            print(\n",
    "                f\"ğŸ“ ì™„ë£Œ ì‹œê°„: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(final_status['completed_at']))}\"\n",
    "            )\n",
    "            print(\n",
    "                f\"ğŸ“ ì²˜ë¦¬ ì‹œê°„: {final_status['completed_at'] - final_status['created_at']:.2f}ì´ˆ\"\n",
    "            )\n",
    "            print(f\"ğŸ“ ZIP íŒŒì¼: {final_status['zip_filename']}\")\n",
    "        elif final_status[\"status\"] == \"failed\":\n",
    "            print(f\"âŒ ì‘ì—… ì‹¤íŒ¨: {final_status.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}\")\n",
    "    except TimeoutError as e:\n",
    "        print(f\"âš ï¸ ì‘ì—… ëŒ€ê¸° ì‹œê°„ ì´ˆê³¼: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ì‘ì—… ìƒíƒœ í™•ì¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. íŒŒì‹± ê²°ê³¼ ë‹¤ìš´ë¡œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-10 03:54:05,517 - teddynote_parser_client - INFO - ì‘ì—… ID '3d541e4d-21ba-4b1c-9c69-d4c82d346422'ì˜ í˜„ì¬ ìƒíƒœ: completed\n",
      "2025-03-10 03:54:05,518 - teddynote_parser_client - INFO - ì‘ì—… ID '3d541e4d-21ba-4b1c-9c69-d4c82d346422'ì˜ ê²°ê³¼ ë‹¤ìš´ë¡œë“œ ì¤‘...\n",
      "2025-03-10 03:54:05,529 - teddynote_parser_client - INFO - ê²°ê³¼ê°€ ì„±ê³µì ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤: client_test_results/3d541e4d-21ba-4b1c-9c69-d4c82d346422_20250310035405.zip\n",
      "2025-03-10 03:54:05,533 - teddynote_parser_client - INFO - ZIP íŒŒì¼ 'client_test_results/3d541e4d-21ba-4b1c-9c69-d4c82d346422_20250310035405.zip'ì˜ ì••ì¶•ì„ 'client_test_results/3d541e4d-21ba-4b1c-9c69-d4c82d346422'ì— í•´ì œí–ˆìŠµë‹ˆë‹¤.\n",
      "2025-03-10 03:54:05,534 - teddynote_parser_client - INFO - ZIP íŒŒì¼ì˜ ì••ì¶•ì´ ì„±ê³µì ìœ¼ë¡œ í•´ì œë˜ì—ˆìŠµë‹ˆë‹¤: client_test_results/3d541e4d-21ba-4b1c-9c69-d4c82d346422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… íŒŒì‹± ê²°ê³¼ê°€ ì„±ê³µì ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤: client_test_results/3d541e4d-21ba-4b1c-9c69-d4c82d346422_20250310035405.zip\n",
      "âœ… ZIP íŒŒì¼ì´ ì„±ê³µì ìœ¼ë¡œ ì••ì¶• í•´ì œë˜ì—ˆìŠµë‹ˆë‹¤: client_test_results/3d541e4d-21ba-4b1c-9c69-d4c82d346422\n",
      "\n",
      "íŒŒì‹± ê²°ê³¼ ë””ë ‰í† ë¦¬: client_test_results/3d541e4d-21ba-4b1c-9c69-d4c82d346422\n"
     ]
    }
   ],
   "source": [
    "# íŒŒì‹± ê²°ê³¼ ë‹¤ìš´ë¡œë“œ ë° ì••ì¶• í•´ì œ\n",
    "if (\n",
    "    \"job_id\" in locals()\n",
    "    and job_id\n",
    "    and \"final_status\" in locals()\n",
    "    and final_status[\"status\"] == \"completed\"\n",
    "):\n",
    "    try:\n",
    "        # ê²°ê³¼ ë‹¤ìš´ë¡œë“œ ë° ì••ì¶• í•´ì œ\n",
    "        zip_path, extract_path = client.download_result(\n",
    "            job_id=job_id, save_dir=RESULTS_DIR, extract=True, overwrite=True\n",
    "        )\n",
    "\n",
    "        print(f\"âœ… íŒŒì‹± ê²°ê³¼ê°€ ì„±ê³µì ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤: {zip_path}\")\n",
    "        print(f\"âœ… ZIP íŒŒì¼ì´ ì„±ê³µì ìœ¼ë¡œ ì••ì¶• í•´ì œë˜ì—ˆìŠµë‹ˆë‹¤: {extract_path}\")\n",
    "        print(f\"\\níŒŒì‹± ê²°ê³¼ ë””ë ‰í† ë¦¬: {extract_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ê²°ê³¼ ë‹¤ìš´ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "        extract_path = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ì‘ì—… ëª©ë¡ ì¡°íšŒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-10 03:54:17,673 - teddynote_parser_client - INFO - ëª¨ë“  ì‘ì—… ëª©ë¡ ì¡°íšŒ ì¤‘...\n",
      "2025-03-10 03:54:17,678 - teddynote_parser_client - INFO - ì´ 1 ê°œì˜ ì‘ì—…ì´ ì¡°íšŒë˜ì—ˆìŠµë‹ˆë‹¤.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‹ ì´ 1ê°œì˜ ì‘ì—…ì´ ìˆìŠµë‹ˆë‹¤.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job ID</th>\n",
       "      <th>Status</th>\n",
       "      <th>Filename</th>\n",
       "      <th>Created At</th>\n",
       "      <th>Completed At</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3d541e4d-21ba-4b1c-9c69-d4c82d346422</td>\n",
       "      <td>completed</td>\n",
       "      <td>2210.03629v3.pdf</td>\n",
       "      <td>2025-03-10 03:53:51</td>\n",
       "      <td>2025-03-10 03:53:59</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Job ID     Status          Filename  \\\n",
       "0  3d541e4d-21ba-4b1c-9c69-d4c82d346422  completed  2210.03629v3.pdf   \n",
       "\n",
       "            Created At         Completed At  \n",
       "0  2025-03-10 03:53:51  2025-03-10 03:53:59  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ì‘ì—… ëª©ë¡ ì¡°íšŒ\n",
    "if api_available:\n",
    "    try:\n",
    "        jobs = client.list_all_jobs()[\"jobs\"]\n",
    "\n",
    "        print(f\"ğŸ“‹ ì´ {len(jobs)}ê°œì˜ ì‘ì—…ì´ ìˆìŠµë‹ˆë‹¤.\\n\")\n",
    "\n",
    "        # ì‘ì—… ì •ë³´ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜\n",
    "        if jobs:\n",
    "            job_data = []\n",
    "            for job in jobs:\n",
    "                created_at = time.strftime(\n",
    "                    \"%Y-%m-%d %H:%M:%S\", time.localtime(job[\"created_at\"])\n",
    "                )\n",
    "                completed_at = (\n",
    "                    time.strftime(\n",
    "                        \"%Y-%m-%d %H:%M:%S\", time.localtime(job[\"completed_at\"])\n",
    "                    )\n",
    "                    if job[\"completed_at\"]\n",
    "                    else \"N/A\"\n",
    "                )\n",
    "\n",
    "                job_data.append(\n",
    "                    {\n",
    "                        \"Job ID\": job[\"job_id\"],\n",
    "                        \"Status\": job[\"status\"],\n",
    "                        \"Filename\": job[\"filename\"],\n",
    "                        \"Created At\": created_at,\n",
    "                        \"Completed At\": completed_at,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            df = pd.DataFrame(job_data)\n",
    "            display(df)\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ì‘ì—… ëª©ë¡ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "def load_documents_from_pkl(filepath):\n",
    "    \"\"\"\n",
    "    Pickle íŒŒì¼ì—ì„œ Langchain Document ë¦¬ìŠ¤íŠ¸ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” í•¨ìˆ˜\n",
    "\n",
    "    Args:\n",
    "        filepath: ì›ë³¸ íŒŒì¼ ê²½ë¡œ (ì˜ˆ: path/to/filename.pdf)\n",
    "    Returns:\n",
    "        Langchain Document ê°ì²´ ë¦¬ìŠ¤íŠ¸\n",
    "    \"\"\"\n",
    "    # í™•ì¥ì ì œê±°í•˜ê³  ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜\n",
    "    abs_path = os.path.abspath(filepath)\n",
    "    base_path = os.path.splitext(abs_path)[0]\n",
    "    pkl_path = f\"{base_path}.pkl\"\n",
    "\n",
    "    with open(pkl_path, \"rb\") as f:\n",
    "        documents = pickle.load(f)\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“„ client_test_results/3d541e4d-21ba-4b1c-9c69-d4c82d346422/3d541e4d-21ba-4b1c-9c69-d4c82d346422/3d541e4d-21ba-4b1c-9c69-d4c82d346422_2210.03629v3.pkl íŒŒì¼ ë¡œë“œ ì¤‘...\n",
      "âœ… ì´ 25ê°œì˜ ë¬¸ì„œê°€ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "# extract_path ë””ë ‰í† ë¦¬ì—ì„œ ëª¨ë“  .pkl íŒŒì¼ ì°¾ê¸°\n",
    "pkl_files = glob.glob(str(Path(extract_path) / \"*\" / \"*.pkl\"))\n",
    "\n",
    "if not pkl_files:\n",
    "    print(\"âŒ extract_pathì—ì„œ .pkl íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "else:\n",
    "    # ëª¨ë“  .pkl íŒŒì¼ì—ì„œ ë¬¸ì„œ ë¡œë“œ\n",
    "    all_documents = []\n",
    "    for pkl_file in pkl_files:\n",
    "        print(f\"ğŸ“„ {pkl_file} íŒŒì¼ ë¡œë“œ ì¤‘...\")  # í•œêµ­ì–´ ì½”ë©˜íŠ¸\n",
    "        documents = load_documents_from_pkl(pkl_file)\n",
    "        all_documents.extend(documents)\n",
    "    \n",
    "    print(f\"âœ… ì´ {len(all_documents)}ê°œì˜ ë¬¸ì„œê°€ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'page': 0, 'source': 'uploads/3d541e4d-21ba-4b1c-9c69-d4c82d346422_2210.03629v3.pdf'}, page_content='# REAC T: SYNERGIZING REASONING AND ACTING IN\\nLANGUAGE MODELS\\nShunyu Yaoâˆ—*,1, Jeffrey Zhao2, Dian Yu2, Nan Du2, Izhak Shafran2, Karthik Narasimhan1, Yuan Cao2\\n1Department of Computer Science, Princeton University\\n2Google Research, Brain team\\n1{shunyuy,karthikn}@princeton.edu\\n2{jeffreyzhao,dianyu,dunan,izhak,yuancao}@google.com\\nABSTRACT\\nWhile large language models (LLMs) have demonstrated impressive performance\\nacross tasks in language understanding and interactive decision making, their\\nabilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action\\nplan generation) have primarily been studied as separate topics. In this paper, we\\nexplore the use of LLMs to generate both reasoning traces and task-speciï¬c actions\\nin an interleaved manner, allowing for greater synergy between the two: reasoning\\ntraces help the model induce, track, and update action plans as well as handle\\nexceptions, while actions allow it to interface with and gather additional information'),\n",
       " Document(metadata={'page': 0, 'source': 'uploads/3d541e4d-21ba-4b1c-9c69-d4c82d346422_2210.03629v3.pdf'}, page_content='from external sources such as knowledge bases or environments. We apply our\\napproach, named ReAct, to a diverse set of language and decision making tasks\\nand demonstrate its effectiveness over state-of-the-art baselines in addition to\\nimproved human interpretability and trustworthiness. Concretely, on question\\nanswering (HotpotQA) and fact veriï¬cation (Fever), ReAct overcomes prevalent\\nissues of hallucination and error propagation in chain-of-thought reasoning by\\ninteracting with a simple Wikipedia API, and generating human-like task-solving\\ntrajectories that are more interpretable than baselines without reasoning traces.\\nFurthermore, on two interactive decision making benchmarks (ALFWorld and\\nWebShop), ReAct outperforms imitation and reinforcement learning methods by\\nan absolute success rate of 34% and 10% respectively, while being prompted with\\nonly one or two in-context examples.\\n# 1 INTRODUCTION'),\n",
       " Document(metadata={'page': 0, 'source': 'uploads/3d541e4d-21ba-4b1c-9c69-d4c82d346422_2210.03629v3.pdf'}, page_content='A unique feature of human intelligence is the ability to seamlessly combine task-oriented actions with\\nverbal reasoning (or inner speech, Alderson-Day & Fernyhough, 2015), which has been theorized to\\nplay an important role in human cognition for enabling self-regulation or strategization (Vygotsky,\\n1987; Luria, 1965; Fernyhough, 2010) and maintaining a working memory (Baddeley, 1992). Con-\\nsider the example of cooking up a dish in the kitchen. Between any two speciï¬c actions, we may\\nreason in language in order to track progress (â€œnow that everything is cut, I should heat up the pot of\\nwaterâ€), to handle exceptions or adjust the plan according to the situation (â€œI donâ€™t have salt, so let\\nme use soy sauce and pepper insteadâ€), and to realize when external information is needed (â€œhow do\\nI prepare dough? Let me search on the Internetâ€). We may also act (open a cookbook to read the\\nrecipe, open the fridge, check ingredients) to support the reasoning and to answer questions (â€œWhat'),\n",
       " Document(metadata={'page': 0, 'source': 'uploads/3d541e4d-21ba-4b1c-9c69-d4c82d346422_2210.03629v3.pdf'}, page_content='dish can I make right now?â€). This tight synergy between â€œactingâ€ and â€œreasoningâ€ allows humans\\nto learn new tasks quickly and perform robust decision making or reasoning, even under previously\\nunseen circumstances or facing information uncertainties.\\nRecent results have hinted at the possibility of combining verbal reasoning with interactive decision\\nmaking in autonomous systems. On one hand, properly prompted large language models (LLMs)\\nhave demonstrated emergent capabilities to carry out several steps of reasoning traces to derive'),\n",
       " Document(metadata={'page': 1, 'source': 'uploads/3d541e4d-21ba-4b1c-9c69-d4c82d346422_2210.03629v3.pdf'}, page_content='Figure 1: (1) Comparison of 4 prompting methods, (a) Standard, (b) Chain-of-thought (CoT,\\nReason Only), (c) Act-only, and (d) ReAct (Reason+Act), solving a HotpotQA (Yang et al., 2018)\\nquestion. (2) Comparison of (a) Act-only and (b) ReAct prompting to solve an AlfWorld (Shridhar\\net al., 2020b) game. In both domains, we omit in-context examples in the prompt, and only show task\\nsolving trajectories generated by the model (Act, Thought) and the environment (Obs).\\nanswers from questions in arithmetic, commonsense, and symbolic reasoning tasks (Wei et al.,\\n2022). However, this â€œchain-of-thoughtâ€ reasoning is a static black box, in that the model uses\\nits own internal representations to generate thoughts and is not grounded in the external world,\\nwhich limits its ability to reason reactively or update its knowledge. This can lead to issues like fact\\nhallucination and error propagation over the reasoning process (Figure 1 (1b)). On the other hand,'),\n",
       " Document(metadata={'page': 1, 'source': 'uploads/3d541e4d-21ba-4b1c-9c69-d4c82d346422_2210.03629v3.pdf'}, page_content='recent work has explored the use of pre-trained language models for planning and acting in interactive\\nenvironments (Ahn et al., 2022; Nakano et al., 2021; Yao et al., 2020; Huang et al., 2022a), with\\na focus on predicting actions via language priors. These approaches usually convert multi-modal\\nobservations into text, use a language model to generate domain-speciï¬c actions or plans, and then\\nuse a controller to choose or execute them. However, they do not employ language models to reason\\nabstractly about high-level goals or maintain a working memory to support acting, barring Huang\\net al. (2022b) who perform a limited form of verbal reasoning to reiterate spatial facts about the\\ncurrent state. Beyond such simple embodied tasks to interact with a few blocks, there have not been\\nstudies on how reasoning and acting can be combined in a synergistic manner for general task solving,\\nand if such a combination can bring systematic beneï¬ts compared to reasoning or acting alone.'),\n",
       " Document(metadata={'page': 1, 'source': 'uploads/3d541e4d-21ba-4b1c-9c69-d4c82d346422_2210.03629v3.pdf'}, page_content='In this work, we present ReAct, a general paradigm to combine reasoning and acting with language\\nmodels for solving diverse language reasoning and decision making tasks (Figure 1). ReAct\\nprompts LLMs to generate both verbal reasoning traces and actions pertaining to a task in an\\ninterleaved manner, which allows the model to perform dynamic reasoning to create, maintain, and\\nadjust high-level plans for acting (reason to act), while also interact with the external environments\\n(e.g. Wikipedia) to incorporate additional information into reasoning (act to reason).'),\n",
       " Document(metadata={'image': '![](file:////app/uploads/3d541e4d-21ba-4b1c-9c69-d4c82d346422_2210.03629v3_Page_1_Index_13.png)', 'entity': '<image>\\n<title>\\nComparative Analysis of Prompting Methods in AI Reasoning\\n</title>\\n<details>\\nThe image illustrates a comparison of four prompting methods used in AI reasoning tasks: Standard, Chain-of-Thought (CoT), Act-Only, and ReAct (Reason + Act). It highlights how these methods approach problem-solving in different scenarios, specifically in HotpotQA and AlfWorld tasks. The ReAct method is emphasized for its ability to interleave reasoning and acting, allowing for dynamic interaction with the environment.\\n</details>\\n<entities>\\nAI prompting methods, HotpotQA, AlfWorld, Apple Remote, Front Row software\\n</entities>\\n<hypothetical_questions>\\n- How might the integration of reasoning and acting improve AI performance in complex tasks?\\n- What challenges could arise from using the ReAct method in real-world applications?\\n</hypothetical_questions>\\n</image>', 'page': 1, 'source': 'uploads/3d541e4d-21ba-4b1c-9c69-d4c82d346422_2210.03629v3.pdf'}, page_content='![](file:////app/uploads/3d541e4d-21ba-4b1c-9c69-d4c82d346422_2210.03629v3_Page_1_Index_13.png)\\n\\n<image>\\n<title>\\nComparative Analysis of Prompting Methods in AI Reasoning\\n</title>\\n<details>\\nThe image illustrates a comparison of four prompting methods used in AI reasoning tasks: Standard, Chain-of-Thought (CoT), Act-Only, and ReAct (Reason + Act). It highlights how these methods approach problem-solving in different scenarios, specifically in HotpotQA and AlfWorld tasks. The ReAct method is emphasized for its ability to interleave reasoning and acting, allowing for dynamic interaction with the environment.\\n</details>\\n<entities>\\nAI prompting methods, HotpotQA, AlfWorld, Apple Remote, Front Row software\\n</entities>\\n<hypothetical_questions>\\n- How might the integration of reasoning and acting improve AI performance in complex tasks?\\n- What challenges could arise from using the ReAct method in real-world applications?\\n</hypothetical_questions>\\n</image>'),\n",
       " Document(metadata={'page': 2, 'source': 'uploads/3d541e4d-21ba-4b1c-9c69-d4c82d346422_2210.03629v3.pdf'}, page_content='We conduct empirical evaluations of ReAct and state-of-the-art baselines on four diverse benchmarks:\\nquestion answering (HotPotQA, Yang et al., 2018), fact veriï¬cation (Fever, Thorne et al., 2018),\\ntext-based game (ALFWorld, Shridhar et al., 2020b), and webpage navigation (WebShop, Yao\\net al., 2022). For HotPotQA and Fever, with access to a Wikipedia API that the model can interact\\nwith, ReAct outperforms vanilla action generation models while being competitive with chain-of-\\nthought reasoning (CoT) (Wei et al., 2022). The best approach overall is a combination of ReAct\\nand CoT that allows for the use of both internal knowledge and externally obtained information\\nduring reasoning. On ALFWorld and WebShop, two or even one-shot ReAct prompting is able\\nto outperform imitation or reinforcement learning methods trained with 103 âˆ¼ 105 task instances,\\nwith an absolute improvement of 34% and 10% in success rates respectively. We also demonstrate'),\n",
       " Document(metadata={'page': 2, 'source': 'uploads/3d541e4d-21ba-4b1c-9c69-d4c82d346422_2210.03629v3.pdf'}, page_content='the importance of sparse, versatile reasoning in decision making by showing consistent advantages\\nover controlled baselines with actions only. Besides general applicability and performance boost,\\nthe combination of reasoning and acting also contributes to model interpretability, trustworthiness,\\nand diagnosability across all domains, as humans can readily distinguish information from modelâ€™s\\ninternal knowledge versus external environments, as well as inspect reasoning traces to understand\\nthe decision basis of model actions.\\nTo summarize, our key contributions are the following: (1) we introduce ReAct, a novel prompt-\\nbased paradigm to synergize reasoning and acting in language models for general task solving; (2) we\\nperform extensive experiments across diverse benchmarks to showcase the advantage of ReAct in a\\nfew-shot learning setup over prior approaches that perform either reasoning or action generation in'),\n",
       " Document(metadata={'page': 2, 'source': 'uploads/3d541e4d-21ba-4b1c-9c69-d4c82d346422_2210.03629v3.pdf'}, page_content='isolation; (3) we present systematic ablations and analysis to understand the importance of acting in\\nreasoning tasks, and reasoning in interactive tasks; (4) we analyze the limitations of ReAct under the\\nprompting setup (i.e. limited support of reasoning and acting behaviors), and perform initial ï¬netuning\\nexperiments showing the potential of ReAct to improve with additional training data. Scaling up\\nReAct to train and operate on more tasks and combining it with complementary paradigms like\\nreinforcement learning could further unlock the potential of large language models.\\n2 REAC T: SYNERGIZING RE ASONING + AC T ING\\nConsider a general setup of an agent interacting with an environment for task solving. At time\\nstep t, an agent receives an observation ot âˆˆ O from the environment and takes an action at âˆˆ A\\nfollowing some policy Ï€(at|ct), where ct = (o1, a1, Â· Â· Â· , otâˆ’1, atâˆ’1, ot) is the context to the agent.'),\n",
       " Document(metadata={'page': 2, 'source': 'uploads/3d541e4d-21ba-4b1c-9c69-d4c82d346422_2210.03629v3.pdf'}, page_content='Learning a policy is challenging when the mapping ct (cid:55)â†’ at is highly implicit and requires extensive\\ncomputation. For example, the agent shown in Figure 1(1c) is unable to generate the correct ï¬nal\\naction (Act 4) to ï¬nish the QA task as it requires complex reasoning over the trajectory context\\n(Question, Act 1-3, Obs 1-3). Similarly, the agent shown in Figure 1(2a) fails to comprehend from the\\ncontext that sinkbasin 1 does not contain peppershaker 1, thus keep producing hallucinating actions.\\nË† = A âˆª L, where L is the\\nThe idea of ReAct is simple: we augment the agentâ€™s action space to A\\nspace of language. An action Ë†at âˆˆ L in the language space, which we will refer to as a thought or a\\nreasoning trace, does not affect the external environment, thus leading to no observation feedback.\\nInstead, a thought Ë†at aims to compose useful information by reasoning over the current context ct,'),\n",
       " Document(metadata={'page': 2, 'source': 'uploads/3d541e4d-21ba-4b1c-9c69-d4c82d346422_2210.03629v3.pdf'}, page_content='and update the context ct+1 = (ct, Ë†at) to support future reasoning or acting. As shown in Figure 1,\\nthere could be various types of useful thoughts, e.g. decomposing task goals and create action plans\\n(2b, Act 1; 1d, Thought 1), injecting commonsense knowledge relevant to task solving (2b, Act 1),\\nextracting important parts from observations (1d, Thought2, 4), track progress and transit action plans\\n(2b, Act 8), handle exceptions and adjust action plans (1d, Thought 3), and so on.\\nHowever, as the language space L is unlimited, learning in this augmented action space is difï¬cult\\nand requires strong language priors. In this paper, we mainly focus on the setup where a frozen\\nlarge language model, PaLM-540B (Chowdhery et al., 2022)1, is prompted with few-shot in-context\\nexamples to generate both domain-speciï¬c actions and free-form language thoughts for task solving\\n(Figure 1 (1d), (2b)). Each in-context example is a human trajectory of actions, thoughts, and'),\n",
       " Document(metadata={'page': 2, 'source': 'uploads/3d541e4d-21ba-4b1c-9c69-d4c82d346422_2210.03629v3.pdf'}, page_content='environment observations to solve a task instance (see Appendix C). For the tasks where reasoning is\\nof primary importance (Figure 1(1)), we alternate the generation of thoughts and actions so that the\\ntask-solving trajectory consists of multiple thought-action-observation steps. In contrast, for decision\\nmaking tasks that potentially involve a large number of actions (Figure 1(2)), thoughts only need to'),\n",
       " Document(metadata={'page': 3, 'source': 'uploads/3d541e4d-21ba-4b1c-9c69-d4c82d346422_2210.03629v3.pdf'}, page_content='appear sparsely in the most relevant positions of a trajectory, so we let the language model decide the\\nasynchronous occurrence of thoughts and actions for itself.\\nSince decision making and reasoning capabilities are integrated into a large language model, ReAct\\nenjoys several unique features: A) Intuitive and easy to design: Designing ReAct prompts is\\nstraightforward as human annotators just type down their thoughts in language on top of their actions\\ntaken. No ad-hoc format choice, thought design, or example selection is used in this paper. We detail\\nprompt design for each task in Sections 3 and 4. B) General and ï¬‚exible: Due to the ï¬‚exible thought\\nspace and thought-action occurrence format, ReAct works for diverse tasks with distinct action\\nspaces and reasoning needs, including but not limited to QA, fact veriï¬cation, text game, and web\\nnavigation. C) Performant and robust: ReAct shows strong generalization to new task instances'),\n",
       " Document(metadata={'page': 3, 'source': 'uploads/3d541e4d-21ba-4b1c-9c69-d4c82d346422_2210.03629v3.pdf'}, page_content='while learning solely from one to six in-context examples, consistently outperforming baselines with\\nonly reasoning or acting across different domains. We also show in Section 3 additional beneï¬ts\\nwhen ï¬netuning is enabled, and in Section 4 how ReAct performance is robust to prompt selections.\\nD) Human aligned and controllable: ReAct promises an interpretable sequential decision making\\nand reasoning process where humans can easily inspect reasoning and factual correctness. Moreover,\\nhumans can also control or correct the agent behavior on the go by thought editing, as shown in\\nFigure 5 in Section 4.\\n# 3 KNOWLEDGE-INTENSIVE REASONING TASKS\\nWe begin with knowledge-intensive reasoning tasks like multi-hop question answering and fact\\nveriï¬cation. As shown in Figure 1(1d), by interacting with a Wikipedia API, ReAct is able to\\nretrieve information to support reasoning, while also use reasoning to target what to retrieve next,\\ndemonstrating a synergy of reasoning and acting.\\n3.1 SETUP'),\n",
       " Document(metadata={'page': 3, 'source': 'uploads/3d541e4d-21ba-4b1c-9c69-d4c82d346422_2210.03629v3.pdf'}, page_content='Domains We consider two datasets challenging knowledge retrieval and reasoning: (1) Hot-\\nPotQA (Yang et al., 2018), a multi-hop question answering benchmark that requires reasoning\\nover two or more Wikipedia passages, and (2) FEVER (Thorne et al., 2018), a fact veriï¬cation\\nbenchmark where each claim is annotated SUPPORTS, REFUTES, or NOT ENOUGH INFO, based\\non if there exists a Wikipedia passage to verify the claim. In this work, we operate in a question-only\\nsetup for both tasks, where models only receive the question/claim as input without access to support\\nparagraphs, and have to rely on their internal knowledge or retrieve knowledge via interacting with\\nan external environment to support reasoning.\\nAction Space We design a simple Wikipedia web API with three types of actions to support\\ninteractive information retrieval: (1) search[entity], which returns the ï¬rst 5 sentences from\\nthe corresponding entity wiki page if it exists, or else suggests top-5 similar entities from the'),\n",
       " Document(metadata={'page': 3, 'source': 'uploads/3d541e4d-21ba-4b1c-9c69-d4c82d346422_2210.03629v3.pdf'}, page_content='Wikipedia search engine, (2) lookup[string], which would return the next sentence in the page\\ncontaining string, simulating Ctrl+F functionality on the browser. (3) finish[answer], which\\nwould ï¬nish the current task with answer. We note that this action space mostly can only retrieve a\\nsmall part of a passage based on exact passage name, which is signiï¬cantly weaker than state-of-the-\\nart lexical or neural retrievers. The purpose is to simulate how humans would interact with Wikipedia,\\nand force models to retrieve via explicit reasoning in language.\\n3.2 METHODS\\nReAct Prompting For HotpotQA and Fever, we randomly select 6 and 3 cases2 from the training\\nset and manually compose ReAct-format trajectories to use as few-shot exemplars in the prompts.\\nSimilar to Figure 1(d), each trajectory consists of multiple thought-action-observation steps (i.e. dense\\nthought), where free-form thoughts are used for various purposes. Speciï¬cally, we use a combination'),\n",
       " Document(metadata={'page': 3, 'source': 'uploads/3d541e4d-21ba-4b1c-9c69-d4c82d346422_2210.03629v3.pdf'}, page_content='of thoughts that decompose questions (â€œI need to search x, ï¬nd y, then ï¬nd zâ€), extract information\\nfrom Wikipedia observations (â€œx was started in 1844â€, â€œThe paragraph does not tell xâ€), perform\\ncommonsense (â€œx is not y, so z must instead be...â€) or arithmetic reasoning (â€œ1844 < 1989â€), guide'),\n",
       " Document(metadata={'page': 4, 'source': 'uploads/3d541e4d-21ba-4b1c-9c69-d4c82d346422_2210.03629v3.pdf'}, page_content='Table 1: PaLM-540B prompting results on\\nHotpotQA and Fever.\\nFigure 2: PaLM-540B prompting results with respect to\\nnumber of CoT-SC samples used.\\nsearch reformulation (â€œmaybe I can search/look up x insteadâ€), and synthesize the ï¬nal answer (â€œ...so\\nthe answer is xâ€). See Appendix C for more details.\\nBaselines We systematically ablate ReAct trajectories to build prompts for multiple baselines (with\\nformats as Figure 1(1a-1c)): (a) Standard prompting (Standard), which removes all thoughts,\\nactions, observations in ReAct trajectories. (b) Chain-of-thought prompting (CoT) (Wei et al.,\\n2022), which removes actions and observations and serve as a reasoning-only baseline. We also\\nbuild a self-consistency baseline (CoT-SC) (Wang et al., 2022a;b) by sampling 21 CoT trajectories\\nwith decoding temperature 0.7 during inference and adopting the majority answer, which is found to\\nconsistently boost performance over CoT. (c) Acting-only prompt (Act), which removes thoughts'),\n",
       " Document(metadata={'page': 4, 'source': 'uploads/3d541e4d-21ba-4b1c-9c69-d4c82d346422_2210.03629v3.pdf'}, page_content='in ReAct trajectories, loosely resembling how WebGPT (Nakano et al., 2021) interacts with the\\nInternet to answer questions, though it operates on a different task and action space, and uses imitation\\nand reinforcement learning instead of prompting.\\nCombining Internal and External Knowledge As will be detail in Section 3.3, we observe that\\nthe problem solving process demonstrated by ReAct is more factual and grounded, whereas CoT\\nis more accurate in formulating reasoning structure but can easily suffer from hallucinated facts\\nor thoughts. We therefore propose to incorporate ReAct and CoT-SC, and let the model decide\\nwhen to switch to the other method based on the following heuristics: A) ReAct â†’ CoT-SC: when\\nReAct fails to return an answer within given steps, back off to CoT-SC. We set 7 and 5 steps for\\nHotpotQA and FEVER respectively as we ï¬nd more steps will not improve ReAct performance3.\\nB) CoT-SC â†’ ReAct: when the majority answer among n CoT-SC samples occurs less than n/2'),\n",
       " Document(metadata={'page': 4, 'source': 'uploads/3d541e4d-21ba-4b1c-9c69-d4c82d346422_2210.03629v3.pdf'}, page_content='times (i.e. internal knowledge might not support the task conï¬dently), back off to ReAct.\\nFinetuning Due to the challenge of manually annotating reasoning traces and actions at scale,\\nwe consider a bootstraping approach similar to Zelikman et al. (2022), using 3,000 trajectories\\nwith correct answers generated by ReAct (also for other baselines) to ï¬netune smaller language\\nmodels (PaLM-8/62B) to decode trajectories (all thoughts, actions, observations) conditioned on\\ninput questions/claims. More details are in Appendix B.1.\\n3.3 RESULTS AND OBSERVATIONS\\nReAct outperforms Act consistently Table 1 shows HotpotQA and Fever results using PaLM-\\n540B as the base model with different prompting methods. We note that ReAct is better than Act\\non both tasks, demonstrating the value of reasoning to guide acting, especially for synthesizing the\\nï¬nal answer, as shown in Figure 1 (1c-d). Fine-tuning results 3 also conï¬rm the beneï¬t of reasoning\\ntraces for more informed acting.'),\n",
       " Document(metadata={'image': '![](file:////app/uploads/3d541e4d-21ba-4b1c-9c69-d4c82d346422_2210.03629v3_Page_4_Index_41.png)', 'entity': \"<image>\\n<title>\\nPerformance Comparison of Prompting Methods in AI Models\\n</title>\\n<details>\\nThe image presents two graphs illustrating the performance of various prompting methods (CoT-SC, ReAct, and CoT) on the HotpotQA and Fever datasets. The left graph shows the accuracy of HotpotQA as the number of CoT-SC trials increases, while the right graph displays the accuracy for the Fever dataset. The lines represent different methods, indicating how each method's performance evolves with more trials.\\n</details>\\n<entities>\\nHotpotQA, Fever, CoT-SC, ReAct, CoT, AI models\\n</entities>\\n<hypothetical_questions>\\n- How might the performance of these prompting methods change with further trials beyond 20?\\n- What factors could influence the effectiveness of each prompting method in different contexts?\\n</hypothetical_questions>\\n</image>\", 'page': 4, 'source': 'uploads/3d541e4d-21ba-4b1c-9c69-d4c82d346422_2210.03629v3.pdf'}, page_content=\"![](file:////app/uploads/3d541e4d-21ba-4b1c-9c69-d4c82d346422_2210.03629v3_Page_4_Index_41.png)\\n\\n<image>\\n<title>\\nPerformance Comparison of Prompting Methods in AI Models\\n</title>\\n<details>\\nThe image presents two graphs illustrating the performance of various prompting methods (CoT-SC, ReAct, and CoT) on the HotpotQA and Fever datasets. The left graph shows the accuracy of HotpotQA as the number of CoT-SC trials increases, while the right graph displays the accuracy for the Fever dataset. The lines represent different methods, indicating how each method's performance evolves with more trials.\\n</details>\\n<entities>\\nHotpotQA, Fever, CoT-SC, ReAct, CoT, AI models\\n</entities>\\n<hypothetical_questions>\\n- How might the performance of these prompting methods change with further trials beyond 20?\\n- What factors could influence the effectiveness of each prompting method in different contexts?\\n</hypothetical_questions>\\n</image>\"),\n",
       " Document(metadata={'table': '| Prompt Methoda | HotpotQA (EM) | Fever (Acc) |\\n| --- | --- | --- |\\n| Standard | 28.7 | 57.1 |\\n| CoT (Wei et al., 2022) | 29.4 | 56.3 |\\n| CoT-SC (Wang et al., 2022a) | 33.4 | 60.4 |\\n| Act | 25.7 | 58.9 |\\n| ReAct | 27.4 | 60.9 |\\n| CoT-SC â†’ ReAct | 34.2 | 64.6 |\\n| ReActâ†’ CoT-SC | 35.1 | 62.0 |\\n| Supervised SoTAb | 67.5 | 89.5 |\\n\\n', 'entity': '', 'page': 4, 'source': 'uploads/3d541e4d-21ba-4b1c-9c69-d4c82d346422_2210.03629v3.pdf'}, page_content='| Prompt Methoda | HotpotQA (EM) | Fever (Acc) |\\n| --- | --- | --- |\\n| Standard | 28.7 | 57.1 |\\n| CoT (Wei et al., 2022) | 29.4 | 56.3 |\\n| CoT-SC (Wang et al., 2022a) | 33.4 | 60.4 |\\n| Act | 25.7 | 58.9 |\\n| ReAct | 27.4 | 60.9 |\\n| CoT-SC â†’ ReAct | 34.2 | 64.6 |\\n| ReActâ†’ CoT-SC | 35.1 | 62.0 |\\n| Supervised SoTAb | 67.5 | 89.5 |\\n\\n\\n\\n'),\n",
       " Document(metadata={'table': '| Prompt Methoda | HotpotQA (EM) | Fever (Acc) |\\n| --- | --- | --- |\\n| Standard | 28.7 | 57.1 |\\n| CoT (Wei et al., 2022) | 29.4 | 56.3 |\\n| CoT-SC (Wang et al., 2022a) | 33.4 | 60.4 |\\n| Act | 25.7 | 58.9 |\\n| ReAct | 27.4 | 60.9 |\\n| CoT-SC â†’ ReAct | 34.2 | 64.6 |\\n| ReActâ†’ CoT-SC | 35.1 | 62.0 |\\n| Supervised SoTAb | 67.5 | 89.5 |\\n\\n', 'entity': '', 'page': 4, 'source': 'uploads/3d541e4d-21ba-4b1c-9c69-d4c82d346422_2210.03629v3.pdf'}, page_content='')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_documents"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
